{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 1\n",
    "# Lending Club Loan Status Analysis\n",
    "## Part 4: Modelling Loan Status: Machine Learning Approach\n",
    "\n",
    "Data Source: Kaggle Dataset -- Lending Club Loan Data  \n",
    "URL: https://www.kaggle.com/wendykan/lending-club-loan-data  \n",
    "Analyst: Eugene Wen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from previous steps.\n",
    "%run ./py/FE.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887379 entries, 0 to 887378\n",
      "Data columns (total 33 columns):\n",
      "funded_amnt                    887379 non-null float64\n",
      "term                           887379 non-null object\n",
      "int_rate                       887379 non-null float64\n",
      "annual_inc                     887379 non-null float64\n",
      "verification_status            887379 non-null object\n",
      "purpose                        887379 non-null object\n",
      "addr_state                     887379 non-null object\n",
      "dti                            887379 non-null float64\n",
      "delinq_2yrs                    887379 non-null float64\n",
      "inq_last_6mths                 887379 non-null float64\n",
      "mths_since_last_delinq         887379 non-null float64\n",
      "mths_since_last_record         887379 non-null float64\n",
      "open_acc                       887379 non-null float64\n",
      "pub_rec                        887379 non-null float64\n",
      "revol_bal                      887379 non-null float64\n",
      "revol_util                     887379 non-null float64\n",
      "total_acc                      887379 non-null float64\n",
      "initial_list_status            887379 non-null object\n",
      "out_prncp                      887379 non-null float64\n",
      "total_pymnt                    887379 non-null float64\n",
      "total_rec_int                  887379 non-null float64\n",
      "total_rec_late_fee             887379 non-null float64\n",
      "recoveries                     887379 non-null float64\n",
      "last_pymnt_amnt                887379 non-null float64\n",
      "collections_12_mths_ex_med     887379 non-null float64\n",
      "mths_since_last_major_derog    887379 non-null float64\n",
      "acc_now_delinq                 887379 non-null float64\n",
      "tot_coll_amt                   887379 non-null float64\n",
      "tot_cur_bal                    887379 non-null float64\n",
      "loan_status_simple             887379 non-null object\n",
      "emp_length_yr                  887379 non-null float64\n",
      "home_owner_s                   887379 non-null object\n",
      "cr_hist_yr                     887379 non-null float64\n",
      "dtypes: float64(26), object(7)\n",
      "memory usage: 223.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Quickly check the dataframe loaded in.\n",
    "loan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Set Preparation\n",
    "As we have seen in the previous section, the target (loan_status_simple) is not balaced and including a level Issued that needs to be removed from the target. This level, however, could be used as new data for prediction. \n",
    "\n",
    "For convenience, we will take the following steps to prepare the training, test and prediction sets:  \n",
    "1. Use custom functions to conduct standardization and dummy coding on the dataframe.\n",
    "2. Split status = Issued as predict set. Undersample status = Good observations to 10% (yield 81149 rows) and append to status = Bad to form train_test_set.\n",
    "3. Split the train_test_set into X_train, y_train, X_test and y_test for further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define std_scaler that takes a dataframe and standardize all numerical columns.\n",
    "# Return updated dataframe.\n",
    "def std_scaler(df):\n",
    "    cols = df.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "    for col in cols: \n",
    "        df[col] = (df[col] - np.mean(df[col]))/np.std(df[col])\n",
    "    return df\n",
    "    \n",
    "# Define dummy_encoder that takes a dataframe and generate (k-1) dummy columns  for all categorical columns.\n",
    "# Return updated dataframe.\n",
    "def dummy_encoder(df):\n",
    "    cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    for col in cols:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(col, axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of loan dataframe\n",
    "loan_copy = loan.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe using custom functions\n",
    "loan_copy = std_scaler(loan_copy)\n",
    "loan_copy = dummy_encoder(loan_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Undersampling the large category by 10%\n",
    "loan_pred = loan_copy.loc[loan_copy.loan_status_simple_Issued == 1, ]\n",
    "loan_pred.drop([\"loan_status_simple_Good\", \"loan_status_simple_Issued\"], axis = 1, inplace=True)\n",
    "\n",
    "loan_bad = loan_copy.loc[(loan_copy.loan_status_simple_Good == 0) & (loan_copy.loan_status_simple_Issued == 0), ]\n",
    "loan_bad[\"isBad\"] = np.abs(loan_bad.loan_status_simple_Good - 1)\n",
    "loan_bad.drop([\"loan_status_simple_Issued\", \"loan_status_simple_Good\"], axis = 1, inplace=True)\n",
    "\n",
    "loan_good = loan_copy.loc[loan_copy.loan_status_simple_Good == 1, ].sample(frac=0.1)\n",
    "loan_good[\"isBad\"] = np.abs(loan_good.loan_status_simple_Good - 1)\n",
    "loan_good.drop([\"loan_status_simple_Issued\", \"loan_status_simple_Good\"], axis = 1, inplace=True)\n",
    "\n",
    "loan_train_test = pd.concat([loan_good, loan_bad], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model debugging\n",
    "loan_train_test = loan_train_test.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_features = loan_train_test.columns.drop(\"isBad\").tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(loan_train_test[all_features], loan_train_test[\"isBad\"], test_size = 0.3, random_state = 1234) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85598923284\n",
      "0.794975325258\n",
      "0.832660385823\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)\n",
    "\n",
    "# Extra Tree Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier()\n",
    "etc.fit(X_train, y_train)\n",
    "predictions2 = etc.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, predictions2)\n",
    "print(accuracy2)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=1234)\n",
    "rfc.fit(X_train, y_train)\n",
    "predictions3 = rfc.predict(X_test)\n",
    "accuracy3 = accuracy_score(y_test, predictions3)\n",
    "print(accuracy3)\n",
    "\n",
    "# Initial model fit\n",
    "\n",
    "#rfc = RandomForestClassifier(random_state=100)\n",
    "#selector = RFECV(rfc, cv=10)\n",
    "#selector.fit(X_train, y_train)\n",
    "#optimized_columns = X.columns[selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance based on Logistic Regression:\n",
      "************************************************\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Feature importance based on Extra Tree Classifier:\n",
      "**************************************************\n",
      "out_prncp                              0.113652\n",
      "last_pymnt_amnt                        0.082206\n",
      "recoveries                             0.065687\n",
      "int_rate                               0.059549\n",
      "total_pymnt                            0.054705\n",
      "total_rec_int                          0.030000\n",
      "funded_amnt                            0.027823\n",
      "revol_util                             0.025431\n",
      "cr_hist_yr                             0.024222\n",
      "dti                                    0.023697\n",
      "tot_cur_bal                            0.023215\n",
      "annual_inc                             0.022623\n",
      "initial_list_status_w                  0.022588\n",
      "total_acc                              0.022061\n",
      "revol_bal                              0.021597\n",
      "open_acc                               0.021460\n",
      "total_rec_late_fee                     0.021251\n",
      "inq_last_6mths                         0.020217\n",
      "emp_length_yr                          0.019672\n",
      "mths_since_last_delinq                 0.019352\n",
      "mths_since_last_major_derog            0.014497\n",
      "term_60                                0.013454\n",
      "tot_coll_amt                           0.012537\n",
      "verification_status_Verified           0.011204\n",
      "home_owner_s_RENT                      0.011014\n",
      "delinq_2yrs                            0.010909\n",
      "mths_since_last_record                 0.010021\n",
      "verification_status_Source Verified    0.009970\n",
      "addr_state_CA                          0.009490\n",
      "pub_rec                                0.009046\n",
      "                                         ...   \n",
      "addr_state_OK                          0.002001\n",
      "addr_state_KY                          0.001998\n",
      "purpose_moving                         0.001824\n",
      "addr_state_UT                          0.001698\n",
      "purpose_medical                        0.001687\n",
      "addr_state_NM                          0.001651\n",
      "addr_state_WV                          0.001620\n",
      "purpose_vacation                       0.001483\n",
      "addr_state_AR                          0.001401\n",
      "addr_state_MS                          0.001337\n",
      "addr_state_HI                          0.001151\n",
      "addr_state_RI                          0.000942\n",
      "purpose_house                          0.000915\n",
      "acc_now_delinq                         0.000871\n",
      "addr_state_MT                          0.000794\n",
      "addr_state_NH                          0.000675\n",
      "addr_state_DE                          0.000631\n",
      "purpose_wedding                        0.000498\n",
      "addr_state_DC                          0.000486\n",
      "purpose_renewable_energy               0.000436\n",
      "purpose_educational                    0.000397\n",
      "addr_state_VT                          0.000374\n",
      "addr_state_SD                          0.000370\n",
      "addr_state_WY                          0.000364\n",
      "addr_state_NE                          0.000163\n",
      "home_owner_s_OTHER                     0.000077\n",
      "addr_state_ND                          0.000075\n",
      "addr_state_ME                          0.000016\n",
      "addr_state_IA                          0.000000\n",
      "addr_state_ID                          0.000000\n",
      "dtype: float64\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Feature importance based on Random Forest Classifier:\n",
      "*****************************************************\n",
      "out_prncp                       0.140705\n",
      "last_pymnt_amnt                 0.124479\n",
      "recoveries                      0.113972\n",
      "total_pymnt                     0.076988\n",
      "int_rate                        0.053959\n",
      "total_rec_int                   0.040759\n",
      "total_rec_late_fee              0.032195\n",
      "annual_inc                      0.029525\n",
      "funded_amnt                     0.029021\n",
      "dti                             0.028829\n",
      "revol_bal                       0.028213\n",
      "revol_util                      0.026039\n",
      "tot_cur_bal                     0.024698\n",
      "cr_hist_yr                      0.022590\n",
      "total_acc                       0.022505\n",
      "open_acc                        0.021206\n",
      "mths_since_last_delinq          0.017714\n",
      "emp_length_yr                   0.014918\n",
      "mths_since_last_major_derog     0.012742\n",
      "tot_coll_amt                    0.011159\n",
      "inq_last_6mths                  0.010088\n",
      "mths_since_last_record          0.009208\n",
      "initial_list_status_w           0.007517\n",
      "delinq_2yrs                     0.006715\n",
      "verification_status_Verified    0.005624\n",
      "term_60                         0.005294\n",
      "pub_rec                         0.005016\n",
      "purpose_debt_consolidation      0.004939\n",
      "home_owner_s_RENT               0.004748\n",
      "addr_state_CA                   0.004247\n",
      "                                  ...   \n",
      "addr_state_NV                   0.000688\n",
      "purpose_vacation                0.000685\n",
      "addr_state_MO                   0.000685\n",
      "addr_state_IN                   0.000649\n",
      "addr_state_WI                   0.000644\n",
      "addr_state_WV                   0.000612\n",
      "addr_state_NH                   0.000537\n",
      "addr_state_DE                   0.000485\n",
      "addr_state_MS                   0.000442\n",
      "purpose_moving                  0.000427\n",
      "addr_state_LA                   0.000414\n",
      "addr_state_RI                   0.000404\n",
      "purpose_house                   0.000343\n",
      "acc_now_delinq                  0.000315\n",
      "purpose_wedding                 0.000312\n",
      "purpose_medical                 0.000278\n",
      "addr_state_NM                   0.000271\n",
      "purpose_educational             0.000244\n",
      "addr_state_DC                   0.000156\n",
      "addr_state_SD                   0.000138\n",
      "addr_state_AR                   0.000138\n",
      "purpose_renewable_energy        0.000132\n",
      "addr_state_VT                   0.000121\n",
      "addr_state_NE                   0.000073\n",
      "addr_state_WY                   0.000051\n",
      "addr_state_ME                   0.000028\n",
      "addr_state_ND                   0.000000\n",
      "addr_state_IA                   0.000000\n",
      "addr_state_ID                   0.000000\n",
      "home_owner_s_OTHER              0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.Series(lr.coef_[0], index=all_features).sort_values(ascending=False)\n",
    "print(\"Feature importance based on Logistic Regression:\")\n",
    "print(\"************************************************\")\n",
    "#print(feature_importance)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "feature_importance2 = pd.Series(etc.feature_importances_, index=all_features).sort_values(ascending=False)\n",
    "print(\"Feature importance based on Extra Tree Classifier:\")\n",
    "print(\"**************************************************\")\n",
    "print(feature_importance2)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "feature_importance3 = pd.Series(rfc.feature_importances_, index=all_features).sort_values(ascending=False)\n",
    "print(\"Feature importance based on Random Forest Classifier:\")\n",
    "print(\"*****************************************************\")\n",
    "print(feature_importance3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_features = feature_importance3[feature_importance3 > 0.05].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813144908031\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=1234)\n",
    "rfc.fit(X_train[opt_features], y_train)\n",
    "predictions_opt = rfc.predict(X_test[opt_features])\n",
    "accuracy_opt = accuracy_score(y_test, predictions_opt)\n",
    "print(accuracy_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning and Pipeline: Choose the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and Tune Different Algorithms: KNN, LR, RF, SVM, XGBoost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model selection function\n",
    "def choose_model(df, feature_list, target_df):\n",
    "    X = df[feature_list]\n",
    "    y = target_df\n",
    "    \n",
    "    dict_list = [\n",
    "        {\n",
    "            \"name\": \"LogisticRegression\",\n",
    "            \"estimator\": LogisticRegression(),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n",
    "                }\n",
    "        },\n",
    "    \n",
    "        {\n",
    "            \"name\": \"KNeighborsClassifier\",\n",
    "            \"estimator\": KNeighborsClassifier(),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"n_neighbors\": range(1,20,2),\n",
    "                    \"weights\": [\"distance\", \"uniform\"],\n",
    "                    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "                    \"p\": [1,2]\n",
    "                }\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"name\": \"RandomForestClassifier\",\n",
    "            \"estimator\": RandomForestClassifier(),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"n_estimators\": [4,6,9],\n",
    "                    \"criterion\":[\"entropy\", \"gini\"],\n",
    "                    \"max_depth\": [2,5,10],\n",
    "                    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "                    \"min_samples_leaf\":[1,5,8],\n",
    "                    \"min_samples_split\":[2,3,5]\n",
    "                }\n",
    "        }]\n",
    "    for dict in dict_list:\n",
    "        print(dict[\"name\"])\n",
    "        grid = GridSearchCV(dict[\"estimator\"], param_grid=dict[\"hyperparameters\"], cv=10)\n",
    "        grid.fit(X, y)\n",
    "        dict[\"best_params\"] = grid.best_params_\n",
    "        dict[\"best_score\"] = grid.best_score_\n",
    "        dict[\"best_estimator\"] = grid.best_estimator_\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "    \n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "{'solver': 'liblinear'}\n",
      "0.78625\n",
      "KNeighborsClassifier\n",
      "{'algorithm': 'brute', 'n_neighbors': 19, 'p': 1, 'weights': 'uniform'}\n",
      "0.807596153846\n",
      "RandomForestClassifier\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 9}\n",
      "0.835673076923\n"
     ]
    }
   ],
   "source": [
    "selected_MLmodel = choose_model(X_train, opt_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add XGBoost and SVM in\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
